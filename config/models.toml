# Model Fleet Configuration for Homunculus
# Defines tiers, providers, escalation rules, and routing signals.
# All model names and thresholds are configured here — not hardcoded in Ruby.

[defaults]
provider = "ollama"                    # Default to local inference
timeout_seconds = 120
max_retries = 2
escalation_enabled = true

# ── Model Tiers ──────────────────────────────────────────────────────

[tiers.whisper]
provider = "ollama"
model = "homunculus-whisper"           # Custom Modelfile (FROM qwen3:4b)
description = "Fast triage, classification, routing"
max_tokens = 512
temperature = 0.3
context_window = 8192

[tiers.workhorse]
provider = "ollama"
model = "homunculus-workhorse"         # Custom Modelfile (FROM qwen3:14b)
description = "General daily tasks, skills, chat"
max_tokens = 4096
temperature = 0.7
context_window = 16384

[tiers.coder]
provider = "ollama"
model = "homunculus-coder"             # Custom Modelfile (FROM qwen2.5-coder:14b)
description = "Code generation, review, debugging"
max_tokens = 8192
temperature = 0.2
context_window = 32768

[tiers.thinker]
provider = "ollama"
model = "homunculus-thinker"           # Custom Modelfile (FROM deepseek-r1:14b)
description = "Deep reasoning, architecture, analysis"
max_tokens = 8192
temperature = 0.6
context_window = 16384

[tiers.cloud_fast]
provider = "anthropic"
model = "claude-haiku-4-5-20251001"
description = "Fast cloud fallback"
max_tokens = 4096
temperature = 0.7

[tiers.cloud_standard]
provider = "anthropic"
model = "claude-sonnet-4-5-20250929"
description = "Code + architecture escalation"
max_tokens = 8192
temperature = 0.7

[tiers.cloud_deep]
provider = "anthropic"
model = "claude-opus-4-6"
description = "Critical decisions, complex reasoning"
max_tokens = 8192
temperature = 0.7

# ── Provider Settings ────────────────────────────────────────────────

[providers.ollama]
base_url = "http://127.0.0.1:11434"
keep_alive = "30m"
health_check_interval_seconds = 60

[providers.anthropic]
# API key comes from ENV["ANTHROPIC_API_KEY"] — NEVER in config
max_tokens_default = 4096

# ── Escalation Rules ────────────────────────────────────────────────

[escalation]
max_local_retries = 3                  # After 3 local failures, escalate to cloud
quality_threshold = 0.3                # If quality score < 0.3, escalate
gibberish_detection = true             # Detect nonsensical responses
cloud_budget_monthly_usd = 30.0        # Hard budget cap
budget_alert_thresholds = [0.5, 0.8, 0.95]  # Alert at 50%, 80%, 95%

# ── Routing Configuration ────────────────────────────────────────────

[routing.skill_map]
paludarium_monitor = "whisper"
infra_sentinel = "whisper"
llm_cost_intel = "whisper"
chinese_companion = "workhorse"
hobonichi_bridge = "workhorse"
weekly_review = "workhorse"
consulting_ops = "workhorse"
financial_autopilot = "workhorse"
ruby_dev = "coder"
git_workflow = "coder"
engineering_leadership = "workhorse"
deep_research = "thinker"
prompt_lab = "workhorse"
self_evolution = "thinker"

[routing.keyword_signals]
code = "coder"
debug = "coder"
refactor = "coder"
ruby = "coder"
docker = "coder"
analyze = "thinker"
architecture = "thinker"
compare = "thinker"
research = "thinker"
